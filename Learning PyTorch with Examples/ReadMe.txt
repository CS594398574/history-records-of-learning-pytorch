pytorch 提供了两个主要的特点：
1、n 维张量，类似于numpy但是能在GPU上运行
2、神经网络构建与训练的自动微分
使用numpy构建了一个FC的神经网络，但是不能用到gpu来对该网络进行加速。所以用tensor。
pytorch张量在概念上与numpy数组相同：张量是一个n维数组，pytorch提供了许多操作这些张量的函数。
在幕后，张量可以跟踪计算图和梯度，但它们也可以作为科学计算的通用工具。
列出了一个手动执行前向后向传播的两层神经网络：

自动微分神经网络FC两层神经网络
计算图中每个tensor代表一个节点。如果x是一个x.requires_grad=true的张量，那么x.grad是另一个保持x相对于某个标量值的梯度的张量
通过定义子类torch.autograd.Function定义自己的自动微分操作并且来执行forward和backward函数
pytorch自动求导有现象tensorflow，框架都是定义一个计算图。并使用自动微分来计算梯度。两者最大的区别在于tensorflow的计算图是静态的，而pytorch使用的是动态计算图。

在TensorFlow中，我们定义一次计算图，然后一遍又一遍地执行同一个图，可能会向该图提供不同的输入数据。在pytorch中，每个向前传递定义一个新的计算图。
静态图是很好的，因为您可以预先优化图形；例如，框架可能决定融合一些图形操作以提高效率，或者制定一个将图形分布在多个GPU或多台机器上的策略。如果您一次又一次地重用同一个图，那么这个潜在的成本高昂的前期优化可以在同一个图一次又一次地重新运行时进行分摊。
静态图和动态图不同的一个方面是控制流。对于某些模型，我们可能希望对每个数据点执行不同的计算；例如，对于每个数据点，可以为不同的时间步数展开循环网络；这种展开可以作为循环实现。对于静态图，循环结构需要是图的一部分；因此，TensorFlow提供了诸如tf.scan之类的运算符，用于将循环嵌入到图中。对于动态图，情况更简单：因为我们为每个示例即时构建图，所以我们可以使用正常的命令流控制来执行针对每个输入的不同计算。

计算图和微分是定义复杂运算符和自动取导数的一个非常强大的范例；但是对于大型神经网络，原始微分可能有点太低级。

在建立神经网络时，我们经常考虑将计算安排成层，其中一些层具有可学习的参数，这些参数在学习过程中会得到优化。

在TensorFlow中，像Keras、TensorFlow Slim和TfLearn这样的包提供了对构建神经网络有用的原始计算图的高级抽象。

在pytorch中，nn包的作用是相同的。神经网络包定义了一组模块，大致相当于神经网络层。模块接收输入张量并计算输出张量，但也可以保持内部状态，例如包含可学习参数的张量。神经网络包还定义了训练神经网络时常用的一组有用的损失函数。

自定义pytroch的 nn 模块(nn Modules)
