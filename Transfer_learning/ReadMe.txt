transfer learning:
运用在3个主要的场景下：
1、ConvNet作为规定的特征提取器。
取一个在imagenet上预训练的convnet，删除最后一个完全连接的层（该层的输出是一个不同任务（如imagenet）的1000个类分数），然后将convnet的其余部分视为新数据集的固定功能提取器。
2、在ConvNet上微调。
第二种策略是在新数据集的convnet上替换和重传分类器，同时通过继续反向传播来微调预训练网络的权重。可以对convnet的所有层进行微调，也可以保持某些早期层的固定（由于过拟合问题），而只对网络的某些高级部分进行微调。
3、预训练模型。

主要常见的4个经验规则：
1、新数据集很小，与原始数据集相似。由于数据很小，因此由于考虑到过度拟合，对convnet进行微调不是一个好主意。由于数据与原始数据相似，我们希望convnet中的更高级功能也与此数据集相关。因此，最好的办法可能是在CNN代码上训练一个线性分类器。

2、新数据集很大，与原始数据集类似。因为我们有更多的数据，我们可以有更多的信心，如果我们试图通过整个网络进行微调，我们将不会过度调整。

3、新数据集很小，但与原始数据集非常不同。因为数据很小，所以最好只训练一个线性分类器。由于数据集非常不同，因此最好不要从网络顶部对分类器进行训练，因为网络包含更多数据集特定的特性。相反，从网络早期的某个激活（activations）中训练SVM分类器可能会更好。

4.新数据集很大，并且与原始数据集非常不同。由于数据集非常大，我们可能期望能够从头开始训练convnet。然而，在实践中，使用预训练模型中的权重进行初始化仍然非常有益。在这种情况下，我们将有足够的数据和信心在整个网络中进行微调。

经验总结：
当新数据很小时候，一般在cnn代码的基础上训练一个线性分类器比较好。
当新数据很大时候，一般可以通过网络进行微调。
实践经验：
来自预训练模型的约束，可能使用新数据集会在网络架构上受到限制。例如不能随意取出conv层。
进行微调时候，普遍使用更小的学习率，因为我们期望ConvNet权重比较好，所以并不希望太快或者太多的扭曲它们。

实验：
1、读取数据。
2、训练模型。
3、微调convNet。
4、convNet作为一个特征提取器。除了最后层，我们可以设置requires_grad==false来冻结其他层参数，使得在反向传播时这些层参数梯度不会再计算。
